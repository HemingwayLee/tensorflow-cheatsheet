# What
* It is TensorFlow's lightweight solution for mobile and embedded devices
  * It enables low-latency inference 
    * a model need to be optimized first
  * With a small binary size
    * because it has no training code 
* It is `NOT` in seperate repo on github, it is in the same repo with TensorFlow
* We need to convert a TensorFlow model to TensorFlow Lite format and optimize it for on-device inference

# vs tensorflow


